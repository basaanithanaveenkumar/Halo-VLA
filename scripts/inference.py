"""
Inference script for Halo-VLA.

Supports two modes:
  1. Interactive  – supply images / state on the command line, type prompts.
  2. Dataset      – run the model over the EO-Data1.5M validation split and
                    report language perplexity + action MSE.

Usage:
    # Interactive (single image, optional state)
    python scripts/inference.py --checkpoint checkpoints/halo_vla_epoch5.pt \
        --image path/to/img.png

    # Dataset evaluation
    python scripts/inference.py --checkpoint checkpoints/halo_vla_epoch5.pt \
        --mode dataset --subset interleave-temporal --batch_size 4
"""

# generated by claude opus 4.6

import argparse
import sys
import time
from pathlib import Path

import torch
import torch.nn.functional as F
from PIL import Image
from torchvision import transforms as T

# ---------------------------------------------------------------------------
# Make sure project root is on sys.path so imports work
# ---------------------------------------------------------------------------
ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(ROOT))
sys.path.insert(0, str(ROOT / "src" / "Halo_VLA"))

from config import HaloVLMConfig
from models.halo_vla import HaloVLM
from dataloader.eo_dataset import build_eo_dataloader

from loguru import logger


# ---------------------------------------------------------------------------
# Checkpoint loading
# ---------------------------------------------------------------------------
def load_checkpoint(ckpt_path: str, device: torch.device):
    """Load model from a training checkpoint."""
    ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)

    # Recover config stored alongside weights
    config = ckpt.get("config", HaloVLMConfig())
    model = HaloVLM(config=config).to(device)
    model.load_state_dict(ckpt["model_state_dict"])
    model.eval()

    logger.info(
        "Loaded checkpoint from {}  (epoch {}, step {})",
        ckpt_path,
        ckpt.get("epoch", -1),
        ckpt.get("global_step", -1),
    )
    return model, config


# ---------------------------------------------------------------------------
# Tokenizer helper
# ---------------------------------------------------------------------------
def get_tokenizer(max_seq_len: int = 512):
    """Return the same tokenizer used during training."""
    from transformers import AutoTokenizer

    tok = AutoTokenizer.from_pretrained(
        "Qwen/Qwen2.5-VL-3B-Instruct", trust_remote_code=True
    )
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token
    return tok


# ---------------------------------------------------------------------------
# Image pre-processing (matches dataloader)
# ---------------------------------------------------------------------------
def preprocess_image(img_path: str, img_size: int = 224) -> torch.Tensor:
    """Load and transform a single image → [1, 3, H, W]."""
    img = Image.open(img_path).convert("RGB")
    transform = T.Compose([
        T.Resize((img_size, img_size)),
        T.ToTensor(),
        T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
    ])
    return transform(img).unsqueeze(0)  # [1, 3, H, W]


# ---------------------------------------------------------------------------
# Greedy auto-regressive generation
# ---------------------------------------------------------------------------
@torch.no_grad()
def generate(
    model: HaloVLM,
    tokenizer,
    images: torch.Tensor,       # [1, N_img, 3, H, W]
    input_ids: torch.Tensor,    # [1, seq_len]
    attention_mask: torch.Tensor,
    states: torch.Tensor,       # [1, N_state, state_dim]
    max_new_tokens: int = 128,
    temperature: float = 1.0,
    top_k: int = 0,
    top_p: float = 1.0,
    eos_token_id: int | None = None,
):
    """
    Auto-regressive greedy / top-k / nucleus generation.

    Returns:
        generated_ids : list[int]  — newly generated token ids
        action_preds  : Tensor or None — action predictions (from last step)
    """
    device = input_ids.device
    config = model.config
    generated_ids: list[int] = []
    action_preds = None

    if eos_token_id is None:
        eos_token_id = tokenizer.eos_token_id

    for _ in range(max_new_tokens):
        logits, action_hiddens = model(
            images=images,
            input_ids=input_ids,
            attention_mask=attention_mask,
            states=states,
        )

        # Take logits at the last position
        next_logits = logits[:, -1, :]  # [1, vocab]

        # Sample actions from flow decoder if hidden states are available
        act_preds = model.sample_actions(action_hiddens) if action_hiddens is not None else None

        # Temperature
        if temperature != 1.0:
            next_logits = next_logits / temperature

        # Top-k filtering
        if top_k > 0:
            top_k_vals, _ = torch.topk(next_logits, top_k)
            threshold = top_k_vals[:, -1].unsqueeze(-1)
            next_logits[next_logits < threshold] = -float("inf")

        # Top-p (nucleus) filtering
        if top_p < 1.0:
            sorted_logits, sorted_idx = torch.sort(next_logits, descending=True)
            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
            remove_mask = cumulative_probs > top_p
            # Shift so the first token above threshold is kept
            remove_mask[:, 1:] = remove_mask[:, :-1].clone()
            remove_mask[:, 0] = False
            indices_to_remove = sorted_idx[remove_mask]
            next_logits[:, indices_to_remove] = -float("inf")

        # Sample or argmax
        if temperature == 0 or (top_k == 0 and top_p >= 1.0):
            next_token = next_logits.argmax(dim=-1, keepdim=True)  # [1, 1]
        else:
            probs = F.softmax(next_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)   # [1, 1]

        token_id = next_token.item()
        generated_ids.append(token_id)

        # Stop on EOS
        if token_id == eos_token_id:
            break

        # Append to sequence for next iteration
        input_ids = torch.cat([input_ids, next_token], dim=1)
        attention_mask = torch.cat(
            [attention_mask, torch.ones(1, 1, dtype=torch.long, device=device)],
            dim=1,
        )

    # Keep last action predictions
    if act_preds is not None:
        action_preds = act_preds

    return generated_ids, action_preds


# ---------------------------------------------------------------------------
# Interactive mode
# ---------------------------------------------------------------------------
def interactive(args):
    device = torch.device(args.device)
    model, config = load_checkpoint(args.checkpoint, device)
    tokenizer = get_tokenizer(max_seq_len=args.max_seq_len)

    # Pre-process images (if supplied)
    img_paths = args.image or []
    if img_paths:
        imgs = [preprocess_image(p, config.img_size) for p in img_paths]
        images = torch.stack([img.squeeze(0) for img in imgs], dim=0).unsqueeze(0).to(device)
        # [1, N_img, 3, H, W]
    else:
        images = torch.zeros(1, 1, 3, config.img_size, config.img_size, device=device)

    N_img = images.size(1)

    # Dummy states (zeros if not provided)
    states = torch.zeros(1, 1, config.state_dim, device=device)

    # Build system prompt
    sys_text = (
        f"<|im_start|>system\n{config.system_prompt}<|im_end|>\n"
    )

    print("\n=== Halo-VLA Interactive Inference ===")
    print(f"  Checkpoint : {args.checkpoint}")
    print(f"  Images     : {len(img_paths)}")
    print(f"  Device     : {device}")
    print("  Type 'quit' or 'exit' to stop.\n")

    while True:
        try:
            user_input = input("User > ").strip()
        except (EOFError, KeyboardInterrupt):
            print()
            break

        if user_input.lower() in ("quit", "exit", "q"):
            break
        if not user_input:
            continue

        # Build prompt: system + user turn, leave assistant open for generation
        image_tags = " ".join(["<image>"] * N_img)
        prompt = (
            f"{sys_text}"
            f"<|im_start|>user\n{image_tags} <state> {user_input}<|im_end|>\n"
            f"<|im_start|>assistant\n"
        )

        encoding = tokenizer(
            prompt,
            max_length=args.max_seq_len,
            truncation=True,
            padding=False,
            return_tensors="pt",
        )
        input_ids = encoding["input_ids"].to(device)
        attention_mask = encoding["attention_mask"].to(device)

        t0 = time.time()
        gen_ids, action_preds = generate(
            model=model,
            tokenizer=tokenizer,
            images=images,
            input_ids=input_ids,
            attention_mask=attention_mask,
            states=states,
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature,
            top_k=args.top_k,
            top_p=args.top_p,
        )
        elapsed = time.time() - t0

        response = tokenizer.decode(gen_ids, skip_special_tokens=True)
        print(f"Assistant > {response}")
        print(f"  [{len(gen_ids)} tokens, {elapsed:.2f}s]")

        if action_preds is not None:
            print(f"  Action predictions shape: {action_preds.shape}")
            print(f"  Action[0]: {action_preds[0, 0].cpu().numpy()}")

        print()


# ---------------------------------------------------------------------------
# Dataset evaluation mode
# ---------------------------------------------------------------------------
@torch.no_grad()
def evaluate_dataset(args):
    device = torch.device(args.device)
    model, config = load_checkpoint(args.checkpoint, device)
    tokenizer = get_tokenizer(max_seq_len=args.max_seq_len)

    loader = build_eo_dataloader(
        subset=args.subset,
        split="train",       # use "train" if no val split is available
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        img_size=config.img_size,
        max_seq_len=args.max_seq_len,
        action_dim=config.action_dim,
        state_dim=config.state_dim,
        shuffle=False,
    )
    logger.info("Eval samples: {}  |  Batches: {}", len(loader.dataset), len(loader))

    total_lang_loss = 0.0
    total_act_loss = 0.0
    total_batches = 0
    t0 = time.time()

    for step, batch in enumerate(loader, 1):
        images = batch["images"].to(device)
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)
        actions = batch["actions"].to(device)
        action_mask = batch["action_mask"].to(device)
        states = batch["states"].to(device)

        logits, action_hiddens = model(
            images=images,
            input_ids=input_ids,
            attention_mask=attention_mask,
            states=states,
        )

        # --- Language loss (cross-entropy) ---
        num_images = (input_ids == config.image_token_id).sum(dim=1).max().item()
        num_patches = (config.img_size // config.patch_size) ** 2
        num_prepended = num_images * num_patches

        text_logits = logits[:, num_prepended:-1, :]
        target = labels[:, 1:]
        min_len = min(text_logits.size(1), target.size(1))
        text_logits = text_logits[:, :min_len, :]
        target = target[:, :min_len]

        lang_loss = F.cross_entropy(
            text_logits.reshape(-1, text_logits.size(-1)),
            target.reshape(-1),
            ignore_index=-100,
        )

        # --- Action loss (MSE via flow matching sample) ---
        act_loss = torch.tensor(0.0, device=device)
        # Sample actions from the flow decoder, then compute MSE against targets
        action_preds = model.sample_actions(action_hiddens) if action_hiddens is not None else None
        if action_preds is not None:
            B_a, n_act, chunk, dim = action_preds.shape
            preds_flat = action_preds.view(B_a, n_act * chunk, dim)
            T = min(preds_flat.size(1), actions.size(1))
            preds_flat = preds_flat[:, :T, :]
            targets = actions[:, :T, :]
            mask = action_mask[:, :T].unsqueeze(-1).float()
            if mask.sum() > 0:
                act_loss = ((preds_flat - targets) ** 2 * mask).sum() / mask.sum() / dim

        total_lang_loss += lang_loss.item()
        total_act_loss += act_loss.item()
        total_batches += 1

        if step % args.log_every == 0:
            logger.info(
                "Step {}/{} | lang={:.4f}  act={:.4f}",
                step, len(loader), lang_loss.item(), act_loss.item(),
            )

    elapsed = time.time() - t0
    avg_lang = total_lang_loss / max(total_batches, 1)
    avg_act = total_act_loss / max(total_batches, 1)
    ppl = torch.exp(torch.tensor(avg_lang)).item()

    logger.info("=" * 60)
    logger.info("Evaluation done in {:.1f}s  ({} batches)", elapsed, total_batches)
    logger.info("  Avg language loss : {:.4f}", avg_lang)
    logger.info("  Perplexity        : {:.2f}", ppl)
    logger.info("  Avg action MSE    : {:.4f}", avg_act)
    logger.info("=" * 60)


# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------
def parse_args():
    p = argparse.ArgumentParser(description="Halo-VLA Inference")

    p.add_argument("--checkpoint", type=str, required=True, help="Path to .pt checkpoint")
    p.add_argument("--mode", choices=["interactive", "dataset"], default="interactive")

    # Interactive-mode options
    p.add_argument("--image", nargs="*", default=[], help="Path(s) to input image(s)")
    p.add_argument("--max_new_tokens", type=int, default=128)
    p.add_argument("--temperature", type=float, default=0.0, help="0 = greedy")
    p.add_argument("--top_k", type=int, default=0)
    p.add_argument("--top_p", type=float, default=1.0)

    # Dataset-mode options
    p.add_argument("--subset", default="interleave-temporal")
    p.add_argument("--batch_size", type=int, default=4)
    p.add_argument("--num_workers", type=int, default=2)

    # Shared
    p.add_argument("--max_seq_len", type=int, default=512)
    p.add_argument("--device", default="cuda" if torch.cuda.is_available() else "cpu")
    p.add_argument("--log_every", type=int, default=10)

    return p.parse_args()


if __name__ == "__main__":
    args = parse_args()
    if args.mode == "interactive":
        interactive(args)
    else:
        evaluate_dataset(args)
